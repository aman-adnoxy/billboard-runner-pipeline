# Adnoxy Data Pipeline ğŸš€

A robust, full-stack data ingestion pipeline designed to standardize, validate, and process billboard inventory data. The system combines a user-friendly **Streamlit** interface with powerful **Prefect** orchestration, using **Supabase** for secure cloud storage.

---

## âœ¨ Key Features

- **Smart Column Mapping**: Auto-detects column names using fuzzy matching and aliases.
- **Dynamic Schema**: Fully configurable via JSON.
- **Advanced Validation**: Checks for missing coordinates, images, and invalid dimensions.
- **Financial Logic**: Automatically calculates Base Rate and Card Rate logic.
- **Modular Architecture**: Clean separation of concerns (UI, Business Logic, Orchestration).

---

## ğŸ“‚ Project Structure

```bash
check-git-main/
â”œâ”€â”€ config/                  # Configuration files
â”‚   â””â”€â”€ standardized_fields.json  # Target schema definition
â”œâ”€â”€ orchestration/           # Prefect Workflow definitions
â”‚   â””â”€â”€ flow.py              # Main execution pipeline
â”œâ”€â”€ src/                     # Core Business Logic
â”‚   â”œâ”€â”€ config.py            # Environment & variable handling
â”‚   â”œâ”€â”€ database.py          # Supabase client wrapper
â”‚   â””â”€â”€ processing.py        # DataFrame transformation & cleaning
â”œâ”€â”€ ui/                      # Frontend Application
â”‚   â””â”€â”€ app.py               # Streamlit Dashboard
â”œâ”€â”€ .env                     # Secrets (Not committed)
â””â”€â”€ requirements.txt         # Python Dependencies
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.10+** installed.
- **Supabase** project (URL and Key).

### Installation

1. **Install Dependencies**
   You can use the provided npm script which auto-installs python requirements:
   ```bash
   npm install
   ```
   *Alternatively, using pure pip:*
   ```bash
   pip install -r requirements.txt
   ```

2. **Environment Setup**
   Create a `.env` file in the root directory:
   ```ini
   SUPABASE_URL="your_supabase_url"
   SUPABASE_KEY="your_supabase_anon_key"
   ```

---

## â–¶ï¸ Usage

Start the application using the consolidated development command:

```bash
npm run dev
```

This will launch the **Streamlit UI** at `http://localhost:8502`.

### Workflow
1. **Upload**: Drag & drop your raw CSV/Excel file.
2. **Map**: Use the UI to map your columns to the standard schema.
   - *Location*: Choose between separate Lat/Long columns or a single Coordinate column.
   - *Dimensions*: Choose between separate W/H columns or a single Dimension string.
3. **Execute**: Click "Save & Run". This uploads the file to Supabase and triggers the Prefect flow.
4. **Monitor**: Click the "View Run in Prefect Cloud" button to track progress.

---

## ğŸ› ï¸ Configuration

The target schema is defined in **`config/standardized_fields.json`**. You can modify this file to change the mapping behavior without touching the code.

**Example Field:**
```json
"billboard_id": {
    "label": "Billboard ID *",
    "aliases": ["id", "code", "serial_no", "asset_id"] 
}
```
- **label**: What the user sees in the UI.
- **aliases**: List of column names to auto-detect and select by default.

---

## ğŸ—ï¸ Architecture Details

- **UI Layer (`ui/`)**: Handles file upload, visual column mapping, and configuration generation. It assumes no heavy processing, merely dispatching instructions.
- **Core Layer (`src/`)**: 
    - `processing.py`: Pure functions for data cleaning (regex, type coercion, pandas logic).
    - `database.py`: Singleton pattern for database connections.
- **Orchestration Layer (`orchestration/`)**: Defines valid Prefect Tasks and Flows. It imports logic from `src/` to keep the flow file clean and readable.

---

## ğŸ“ License

Proprietary/Internal Use.
